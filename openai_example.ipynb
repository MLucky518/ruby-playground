{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI SDK Example in IRuby\n",
    "This notebook demonstrates using the `ruby-openai` gem from an IRuby (Jupyter) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Faraday::UnauthorizedError",
     "evalue": "the server responded with status 401 for POST https://api.openai.com/v1/chat/completions",
     "output_type": "error",
     "traceback": [
      "\u001b[31mFaraday::UnauthorizedError\u001b[0m: the server responded with status 401 for POST https://api.openai.com/v1/chat/completions",
      "/usr/local/bundle/gems/faraday-2.13.2/lib/faraday/response/raise_error.rb:30:in `on_complete'",
      "/usr/local/bundle/gems/faraday-2.13.2/lib/faraday/middleware.rb:57:in `block in call'",
      "/usr/local/bundle/gems/faraday-2.13.2/lib/faraday/response.rb:42:in `on_complete'",
      "/usr/local/bundle/gems/faraday-2.13.2/lib/faraday/middleware.rb:56:in `call'",
      "/usr/local/bundle/gems/faraday-2.13.2/lib/faraday/rack_builder.rb:153:in `build_response'",
      "/usr/local/bundle/gems/faraday-2.13.2/lib/faraday/connection.rb:452:in `run_request'",
      "/usr/local/bundle/gems/faraday-2.13.2/lib/faraday/connection.rb:280:in `post'",
      "/usr/local/bundle/gems/ruby-openai-8.1.0/lib/openai/http.rb:22:in `json_post'",
      "/usr/local/bundle/gems/ruby-openai-8.1.0/lib/openai/client.rb:24:in `chat'",
      "(irb):2:in `<top (required)>'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://localhost:8888/'. Verify the server is running and reachable."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "require 'openai'\n",
    "client = OpenAI::Client.new(access_token: ENV[\"OPENAI_API_KEY\"])\n",
    "response = client.chat(parameters: {\n",
    "  model: \"gpt-3.5-turbo\",\n",
    "  messages: [{ role: \"user\", content: \"Say hello from Ruby!\" }]\n",
    "})\n",
    "puts response.dig(\"choices\", 0, \"message\", \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-\n"
     ]
    }
   ],
   "source": [
    "puts ENV['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 3 (iruby kernel)",
   "language": "ruby",
   "name": "ruby3"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "3.2.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
